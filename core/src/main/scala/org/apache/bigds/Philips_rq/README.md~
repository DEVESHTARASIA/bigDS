Trumpet
=======

Project Trumpet is a comprehensive collection of Big Data Workloads and framework to run, orchestrate and analyze a used case based workloads for various Big Data frameworks.

Install
=======

You need to install Spark 1.2.0 or higher versions together with hadoop 1.0.4 as storage support.

  
Building Trumpet
----------------

Clone Trumpet from github:

    cd [your_building_root]
    git clone git@github.com:intel-hadoop/Trumpet.git
  
Build & run:

    sbt                  // in Trumpet root
    project Trumpet      // select Trumpet project
    compile              // compile the project
    run ... ... ...      // run Trumpet
   
Run Trumpet
-----------

Currently, multiple main defined for various workload in same project. So you will need to select one:


    [1] com.intel.genbase.trumpet.SVD
    [2] com.intel.genbase.trumpet.Covariance
    [3] com.intel.genbase.trumpet.generator.GenBaseData
    [4] com.intel.genbase.trumpet.generator.GenSQLLoadScript
    [5] com.intel.genbase.trumpet.LinearRegression
    [6] com.intel.genbase.trumpet.WilcoxonRankSum

For [1], [5], you need to provide [spark master address] [path to directory of generated data] [nparts] [function filter] [gene table file name] [geo table file name] [go table file name] [patient table file name]. 

    For example:
    run local[2] hdfs://localhost:8020/data_gen 256 250 gene.csv geo.csv go.csv patient.csv 
    1                         // Select SVD

  output:
  
    SingularValueDecomposition(null,[4820566.209871132,1951034.398640123,1774588.8071315559,749934.0405079712,439065.41586629796],-0.45649065686842416  0.10704783856803027  -0.5534740228643926   ... (5 total)
    -0.5080852734133808   0.06139460014273009  -0.36800441416895097  ...
    -0.5568322450546807   0.16973095360880952  0.7447760130375791    ...
    -0.3748323486770206   0.25858025935288964  0.0249192238524035    ...
    -0.287930888209163    -0.9429205248541701  0.05410156978235717   ...)

For [2], you will need to provide [spark master address] [path to directory of generated data] [nParts] [disease filter] [gene table file name] [geo table file name] [go table file name] [patient table file name].

    For example: 
    run local[2] hdfs://localhost:8020/data_gen 256 5 gene.csv geo.csv go.csv patient.csv 
    2                        // Select Covariance
[function filter] [disease filter] [age> <gender] are data filters for diffirent algorithms. For example, in data-preprocessing of SVD, Spark will only choose genes with function number smaller than [function filter]. In covariance, Spark will only choose patients with a certain disease category as indicated by [disease filter]. In 
biclustering, Spark will only choose patients of certain gender and younger than a certain age as indicated by [age] [gender]. 

The meanings of the parameters mentioned above are indicated in the configuration file conf/trumpet-deployment-conf. 


Deploy to Spark cluster
-----------------------

First, build jar package, in SBT:

    project Trumpet
    package

Edit "conf/trumpet-deployment-conf" to supply SPARK_MASTER & HDFS address, data size, batch number, filename, algorithm parameters (uncomment the variables to set numbers and names according to your personal need, otherwise default values are imposed):

		##SERVER##
		SPARK_MASTER=<spark master address:port>
		HDFS_MASTER=<hdfs master address:port>
		DATASET_FOLDER=<data folder in hdfs>

		##DATA SIZE##
		nParts=2048
		nCol=100000
		nRow=60000
		#Generate a large data set batch by batch so that it will not be sqeezed in the memory as a whole.
		#BATCH_NUM is the number of batches. The larger BATCH_NUM is, the smaller memory it will be for 
		#generating data sets. 
		BATCH_NUM=1 


		##FILE NAME##
		#GENE=gene.csv
		#GEO=geo.csv
		#GO=go.csv
		#PATIENT=patient.csv
		nnodes=8

		##ALGORITHM PARAMETER##
		#==BiCluster== Biclustering will only be imposed to patients with certain gender and younger than certain age. 
		#age=40
		#gender=1

		#==Covariance== Covariance will only be imposed to patients with certain disease number. 
		#disease_filter=5

		#==Linear regression/SVD== Linear regression and SVD will only be imposed to genes with function numbers 
                #smaller than the number indicated. 
		#function_filter=250


Execute "gen_data_set.sh" to generate data, and "run_xxx" for corresponding cases.

	Before run "run_xxx.sh", please make sure the current file names you set is the same as the file names saved in your hdfs. And please make sure that your data has been successfully generated. 
